= Elaticsearch 文档API

== 读写模型

=== 介绍
elasticsearch中的索引被分成多个分片,每个分片又存在多个副本.这些副本称为复制组，在添加或删除文档时必须保持同步.如果我们不这样做，从一个副本中读取的数据可能与从另一个副本中读取的结果截然不同。保持分片副本同步并从中提供读取的过程就是我们所说的数据复制模型。

Elasticsearch的数据复制模型基于主备份模型，并在Pacific Research的Microsoft Research论文中得到了很好的描述。该模型基于具有来自复制组(主分片和所有副本分片称为单个复制组)的单个副本，该副本充当主要分片。其他副本称为副本分片。主分片作为所有索引操作的主要入口点。它负责验证它们并确保它们是正确的。一旦主分片接受了索引操作，主分片负责将操作复制到其他副本。

=== 写模型

Elasticsearch中的每个索引操作首先使用路由解析为复制组，通常基于文档ID。确定复制组后，操作将在内部被转发到复制组中的当前主分片。主分片负责验证操作并将其转发到其他副本。由于副本可以脱机，因此不需要将主副本复制到所有副本。相反，Elasticsearch维护应该接收该操作的分片副本列表.此列表称为同步副本，由主节点维护。顾名思义，这些是“好”分片副本的集合，保证已经处理了已经向用户确认的所有索引和删除操作。主分片负责维护此不变量，因此必须将所有操作复制到此集合中的每个副本。

主分片遵循以下基本流程：
. 验证传入操作并在结构无效时拒绝它（例如：有一个对象字段，其中包含一个数字）
. 在本地执行操作，即索引或删除相关文档。 这也将验证字段的内容并在需要时拒绝（例如：关键字值太长，无法在Lucene中进行索引）。
. 将操作转发到当前同步副本集中的每个副本。如果有多个副本，则这是并行完成的。
. 一旦所有副本成功执行了操作并响应主服务器，主服务器响应客户端,返回操作成功的结果。

==== 失败处理

在索引编制过程中可能会出现许多问题 - 磁盘可能会损坏，节点可能会相互断开连接，或者某些配置错误可能导致复制副本上的操作失败，尽管它在主分片上成功。 这些很少见，但主分片必须处理它们。

在主分片本身发生故障的情况下,其宿主机器会向mater节点报告此信息.索引操作将等待(默认情况下最多一分钟),以便mater节点将其中一个副本提升为新的主分片.然后,该操作将会被转到新的主分片.请注意,master节点还会监控其他节点的状态,并可能决定主动降级主分片.这种情况通常发生在主分片的宿主节点因为网络故障脱离集群的时候.

一旦主分片上成功执行了操作,主分片就必须在副分片上执行他时处理潜在的故障.这可能是副本在执行时发生了故障或者网络问题请求无法到达副本(或者副本无法成功返回响应).为了避免违反不变量,主分片上master节点发送请求下线有问题的分片.只有master确认删除了问题分片后,主分片才会确认成功执行了操作.请注意.master还将指示另一个节点开始构建新的分片副本,以便将系统恢复到正常状态.

在将操作转发到副本时，主分片将使用副本来验证它仍然是活动分片。 如果主要由于网络分区（或长GC）而被隔离，则它可能会在意识到它已被降级之前继续处理传入的索引操作。副本分片将拒绝来自陈旧主分片的操作。 当主分片收到来自副本的拒绝其请求响应时，那么它将联系master并将知道它已被替换。 然后将操作路由到新主分片。

[NOTE]
====
如果没有副本会发生什么?

在这种情况下,主分片处理操作,但是没有任何外部验证,这看起来是有问题的.另一方面,主分片不能使其他分片失败,会请求master代表它执行操作,这意味着master知道主分片是唯一的良好分片.因此,我们保证master不会将任何其他副本分片提升为新的主分片,并且任何发送到主分片的操作都不会丢失. 当然，由于此时我们只使用单个数据副本运行，因此物理硬件问题可能会导致数据丢失
====

=== 读模型

Elasticsearch中的读取可以是根据ID的非常轻量级的查找，也可以是具有复杂聚合的大量搜索请求，这些聚合会占用非常多的CPU。 主备模型的一个优点是它使所有分片副本保持相同（除了正在进行的操作）。 因此，单个同步副本足以提供读取请求。

当节点收到读取请求时，该节点负责将其转发到保存相关分片的节点，整理响应并响应客户端。 我们将该节点称为该请求的协调节点。 基本流程如下：
. 将读取请求解析为相关分片。 请注意，由于大多数搜索将被发送到一个或多个索引，因此它们通常需要从多个分片中读取，每个分片代表数据的不同子集。
. 从分片复制组中选择每个相关分片的活动副本。 这可以是主分片或副本。默认情况下，Elasticsearch将简单地在分片副本之间循环。
. 将分片级读取请求发送到所选副本。
. 整合结果并做出回应。 请注意，在通过ID查找的情况下，只有一个分片是相关的，可以跳过此步骤。

==== 失败处理
当分片无法响应读取请求时，协调节点将从同一复制组中选择另一个副本，并将分片级别搜索请求发送到该副本。重复失败可能导致没有可用的分片副本。 在某些情况下，例如_search，Elasticsearch更愿意快速响应而不是等待问题得到解决，这样会返回部分结果（部分结果显示在响应的_shards标头中）。

== Index API

Index API添加或者更新特定索引下的一个typed Json文档,使该文档可以被搜索,下面的例子在twitter索引下面创建一个type为_doc的文档,文档id为1:
[source,shell]
----
PUT twitter/_doc/1
{
    "user" : "kimchy",
    "post_date" : "2009-11-15T14:12:12",
    "message" : "trying out Elasticsearch"
}
---- 
响应结果是:
[source,json]
----
{
    "_shards" : { <1>
        "total" : 2, <2>
        "failed" : 0, <3>
        "successful" : 2 <4>
    },
    "_index" : "twitter",
    "_type" : "_doc",
    "_id" : "1",
    "_version" : 1,
    "_seq_no" : 0,
    "_primary_term" : 1,
    "result" : "created"
}
----

<1> 提供索引复制进程的结果
<2> 表明分片应该被复制的数量
<3> 如果索引复制失败,返回包含错误信息的数组
<4> 索引分片被复制成功的数量,索引操作成功,改值至少为1

NOTE: 索引操作成功返回时，副本分片并非完全成功,在这种情况下,total将等于基于number_of_replicas设置的总分片数，并且successful将等于已启动的分片数（主副本和副本）,如果没有失败,failed 为0.

=== 自动创建索引
如果索引尚不存在，则索引操作会自动创建索引，并应用已配置的任何索引模板。索引操作还会为指定的类型创建动态类型映射（如果尚不存在）。 默认情况下，如果需要，新字段和对象将自动添加到指定类型的映射定义中。

自动索引创建由action.auto_create_index设置控制。 此设置默认为true，表示始终自动创建索引。通过将此设置的值更改为逗号分隔的模式列表，表示仅允许对匹配特定模式的索引创建自动索引。它也可以通过在列表中用+或-前缀来明确允许和禁止。 最后，通过将此设置更改为false，可以完全禁用它。
[source,shell]
----
PUT _cluster/settings
{
    "persistent": {
        "action.auto_create_index": "twitter,index10,-index1*,+ind*" 
    }
}

PUT _cluster/settings
{
    "persistent": {
        "action.auto_create_index": "false" 
    }
}

PUT _cluster/settings
{
    "persistent": {
        "action.auto_create_index": "true" 
    }
}
----

==== 操作类型
索引操作还接受可用于强制创建操作的op_type参数，允许“put-if-absent”行为.op_type=create时，如果索引中已存在该id的文档，则索引操作将失败。
[source,shell]
----
PUT twitter/_doc/1?op_type=create
{
    "user" : "kimchy",
    "post_date" : "2009-11-15T14:12:12",
    "message" : "trying out Elasticsearch"
}
----
也可以写成下面的形式
[source,shell]
----
PUT twitter/_doc/1/_create
{
    "user" : "kimchy",
    "post_date" : "2009-11-15T14:12:12",
    "message" : "trying out Elasticsearch"
}
----

==== ID自动生成
可以在不指定id的情况下执行索引操作。在这种情况下，将自动生成id。此外，op_type将自动设置为create。这是一个例子（注意POST使用而不是PUT）：
[source,shell]
----
POST twitter/_doc/
{
    "user" : "kimchy",
    "post_date" : "2009-11-15T14:12:12",
    "message" : "trying out Elasticsearch"
}
----
响应如下:
[source,json]
----
{
    "_shards" : {
        "total" : 2,
        "failed" : 0,
        "successful" : 2
    },
    "_index" : "twitter",
    "_type" : "_doc",
    "_id" : "W0tpsmIBdwcYyG50zbta",
    "_version" : 1,
    "_seq_no" : 0,
    "_primary_term" : 1,
    "result": "created"
}
----
==== 乐观并发控制

索引操作可以是可选的，只有在为文档的最后一次修改分配了if_seq_no和if_primary_term参数指定的序列号和主要术语时才能执行索引操作。如果检测到不匹配，则操作将导致VersionConflictException和状态代码409.有关详细信息，请参阅乐观并发控制。

====  路由

默认情况下,索引选择存储的分片(路由)是根据文档ID的hash值确定的.为了更自由的控制，可以在索引操作上使用routing参数,该参数值作为散列函数的输入值.
[source,shell]
----
POST twitter/_doc?routing=kimchy
{
    "user" : "kimchy",
    "post_date" : "2009-11-15T14:12:12",
    "message" : "trying out Elasticsearch"
}
----

当显示设置mapping时,_routing字段可以用于指示索引操作从文档本身提取路由值。这会多出一个文档解析的成本(非常小),如果指定了_routing并设置为必须,在索引的过程中,如果文档缺少该字段则会导致索引操作失败.

**分布式**

索引操作根据其路由指向主分片（请参阅上面的“路由”部分），并在包含此分片的实际节点上执行。 主分片完成操作后，如果需要，更新将分发到适用的副本。

==== 等待活动状态分片

为了提高写入系统的弹性,索引操作在执行之前,会等待一定数量的活动状态分片.如果等待的活动状态分片数量不够,写操作会等待和重试,直到满足要求的活动状态分片或者超时.默认,写操作只等待主分片处于激活状态,这个可以通过index.write.wait_for_active_shards设置.如果要在指定的单独操作生效,可以追加wait_for_active_shards参数.该参数的值可以指定为all,必须为正数且不能大于副本总数.

假如我们有一个三个节点(A,B,C)的集群,副本数设置为3,那么主分片加副本总共有4个.如果我们尝试执行索引操作,默认只会在执行前确认主分片状态是活动的.这意味着即使B和C节点故障,A节点上的主分片仍然可以执行操作,只是仅有一个副本.如果wait_for_active_shards被设置为3,索引操作会要求三个副本分片都处于活动状态.如果我们将wait_for_active_shards设置为all（或者4，它是相同的），则索引操作将不会继续，因为我们没有在索引中激活每个分片的所有4个副本。 除非在群集中启动新节点以托管分片的第四个副本，否则操作将超时。


重要的是要注意，此设置大大降低了写操作没有写入必要数量的分片副本的可能性，但它并没有完全消除这种可能性，因为这种检查发生在写操作开始之前.一旦写入操作正在进行，复制在任何数量的分片副本上仍然可能失败，但可以在主要副本上成功。写操作响应的_shards部分显示复制成功/失败的分片副本数。
[source,json]
----
{
    "_shards" : {
        "total" : 2,
        "failed" : 0,
        "successful" : 2
    }
}
----

**refresh**
控制此请求所做的更改何时对搜索可见。请参阅刷新。

**Noop 更新**

使用索引API更新文档时，即使文档未更改，也始终会创建新版本的文档.如果这是不可接受的，请使用_update API，并将detect_noop设置为true。此选项在索引API上不可用，因为索引API不会获取旧源，也无法将其与新源进行比较。

**超时**
执行索引操作时，分配用于执行索引操作的主分片可能不可用。原因可能是主分片当前正从网关恢复或正在进行重定位.默认情况下，索引操作将在主分片上等待最多1分钟，然后失败并响应错误。timeout参数可用于显式指定等待的时间。以下是将其设置为5分钟的示例：
[source,shell]
----
PUT twitter/_doc/1?timeout=5m
{
    "user" : "kimchy",
    "post_date" : "2009-11-15T14:12:12",
    "message" : "trying out Elasticsearch"
}
----

**版本号**

每个索引文档都有一个版本号。 默认情况下，使用从1开始的内部版本控制，并在每次更新时递增，包括删除。 可选地，版本号可以设置为外部值（例如，如果在数据库中维护）。要启用此功能，应将version_type设置为external。提供的值必须是大于或等于0且小于大约9.2e + 18的数字长值。

使用外部版本类型时，系统会检查传递给索引请求的版本号是否大于当前存储文档的版本号。如果为true，则将索引文档并使用新版本号。如果提供的值小于或等于存储文档的版本号，则会发生版本冲突，索引操作将失败。 例如：
[source,shell]
----
PUT twitter/_doc/1?version=2&version_type=external
{
    "message" : "elasticsearch now has versioning support, double cool!"
}
----

NOTE: 版本控制是完全实时的，不受搜索操作的近实时方面的影响。 如果未提供任何版本，则执行该操作而不进行任何版本检查

由于提供的版本2高于当前文档版本1，因此上述操作将成功。如果文档已更新且其版本设置为2或更高，则索引命令将失败并导致冲突（409 http 状态代码）。

WARNNING: 外部版本控制支持值0作为有效版本号。 这允许版本与外部版本控制系统同步，其中版本号从零开始而不是从一开始。 它具有副作用，版本号等于零的文档既不能使用Update By Query API更新，也不能使用Delete By Query API删除。

除了external版本类型,还支持以下几种:

. internal:仅在给定版本与存储文档的版本相同时才对文档编制索引。
. external或者external_gt:如果给定版本严格高于存储文档的版本或者没有该文档，则索引文档。给定版本将用作新版本，并将与新文档一起存储。 提供的版本必须是非负long类型。
. external_gte:仅在给定版本等于或高于存储文档的版本时索引文档。 如果没有现有文档，操作也将成功。给定版本将用作新版本，并将与新文档一起存储。 提供的版本必须是非负long类型。

== GET API
[source,shell]
----
GET twitter/_doc/0
----
[source,json]
----
{
    "_index" : "twitter",
    "_type" : "_doc",
    "_id" : "0",
    "_version" : 1,
    "_seq_no" : 10,
    "_primary_term" : 1,
    "found": true,
    "_source" : {
        "user" : "kimchy",
        "date" : "2009-11-15T14:12:12",
        "likes": 0,
        "message" : "trying out Elasticsearch"
    }
}
----

查看文档是都存在:
[source,shell]
----
HEAD twitter/_doc/0
----

**实时**
默认情况下，get API是实时的，并且不受索引刷新频率的影响（当数据对搜索可见时）。 如果文档已更新但尚未刷新，则get API将就地发出刷新调用以使文档可见。这也将使上次刷新后发生变化的文档可见。为了禁用实时GET，可以将realtime参数设置为false。

**结果字段过滤**
默认情况下，除非已使用stored_fields参数或禁用了_source字段，否则get操作将返回_source字段的内容。您可以使用_source参数关闭_source检索：
[source,shell]
----
GET twitter/_doc/0?_source=false
----
如果您只需要完整_source中的一个或两个字段，则可以使用_source_includes和_source_excludes参数来包含或过滤掉您需要的部分。这对于大型文档尤其有用，其中部分检索可以节省网络开销。 这两个参数都使用逗号分隔的字段列表或通配符表达式。 例：
[source,shell]
----
GET twitter/_doc/0?_source_includes=*.id&_source_excludes=entities
----

如果您只想指定包含，则可以使用较简洁的表示法：
[source,shell]
----
GET twitter/_doc/0?_source=*.id,retweeted
----

**存储的字段**

get操作允许指定一组存储的字段，这些字段将通过传递给stored_fields参数返回。如果未存储请求的字段，则忽略它们。例如，考虑以下映射：
[source,shell]
----
PUT twitter
{
   "mappings": {
      "_doc": {
         "properties": {
            "counter": {
               "type": "integer",
               "store": false
            },
            "tags": {
               "type": "keyword",
               "store": true
            }
         }
      }
   }
}
----
现在,创建文档
[source,shell]
----
PUT twitter/_doc/1
{
    "counter" : 1,
    "tags" : ["red"]
}
----
现在检索文档:
[source,shell]
----
GET twitter/_doc/1?stored_fields=tags,counter
----
结果如下:
[source,json]
----
{
   "_index": "twitter",
   "_type": "_doc",
   "_id": "1",
   "_version": 1,
   "_seq_no" : 22,
   "_primary_term" : 1,
   "found": true,
   "fields": {
      "tags": [
         "red"
      ]
   }
}
----
从文档本身获取的字段值始终作为数组返回。 由于未存储counter字段，因此在尝试获取stored_field时，get请求会忽略它。

也可以检索像_routing字段这样的元数据字段：
[source,shell]
---
PUT twitter/_doc/2?routing=user1
{
    "counter" : 1,
    "tags" : ["white"]
}
---

[source,shell]
----
GET twitter/_doc/2?routing=user1&stored_fields=tags,counter
----

结果如下:
[source,json]
----
{
   "_index": "twitter",
   "_type": "_doc",
   "_id": "2",
   "_version": 1,
   "_seq_no" : 13,
   "_primary_term" : 1,
   "_routing": "user1",
   "found": true,
   "fields": {
      "tags": [
         "white"
      ]
   }
}
----
此外，只能通过stored_field选项返回叶子字段。因此无法返回对象字段，此类请求将失败。

**直接获取_source**
使用/{index}/{type}/{id}/_source端点只获取文档的_source字段，而不包含任何其他内容。 例如：
[source,shell]
----
GET twitter/_doc/1/_source
----
您还可以使用相同的源过滤参数来控制将返回_source的哪些部分：
[source,shell]
----
GET twitter/_doc/1/_source?_source_includes=*.id&_source_excludes=entities
----

注意，_source端点还有一个HEAD变体，可以有效地测试document _source的存在。如果在映射中禁用了现有文档，则该文档将没有_source。
[source,shell]
----
HEAD twitter/_doc/1/_source
----

**路由**
索引文档的时候使用了路由字段,在检索的时候,也必须提供该字段:
[source,shell]
----
GET twitter/_doc/2?routing=user1
----

请注意，在没有正确路由的情况下发出get将导致无法获取文档。

**优先级**
preference参数控制那个副本执行查询请求,默认是随机的,该值可以设为:
. _primary:该操作将仅在主分片上执行。
. _local:如果可能，操作将优选在本地分配的分片上执行。
. 自定义值:将使用自定义值来保证相同的分片将用于相同的自定义值。 当在不同的刷新状态下击中不同的分片时，这可以帮助“跳跃值”。 示例值可以是Web会话ID或用户名。

**刷新**
可以将refresh参数设置为true，以便在get操作之前刷新相关的分片并使其可搜索。 将其设置为true应该在仔细考虑并验证这不会导致系统负载过重（并减慢索引速度）之后进行。

**分布式**
get操作被散列为特定的分片ID。然后它被重定向到该分片ID中的一个副本并返回结果。副本是主分片及其在该分片ID组中的副本。这意味着我们拥有的副本越多，我们将拥有更好的GET伸缩能力。

== DELETE API
[source,shell]
----
DELETE /twitter/_doc/1
----
响应如下:
[source,json]
----
{
    "_shards" : {
        "total" : 2,
        "failed" : 0,
        "successful" : 2
    },
    "_index" : "twitter",
    "_type" : "_doc",
    "_id" : "1",
    "_version" : 2,
    "_primary_term": 1,
    "_seq_no": 5,
    "result": "deleted"
}
----

== Delete By Query API
最简单的_delete_by_query用法只会对匹配查询的每个文档执行删除操作。 这是API：
[source,shell]
----
POST twitter/_delete_by_query
{
  "query": {  <1>
    "match": {
      "message": "some message"
    }
  }
}
----
<1> 必须以与Search API相同的方式将查询作为值传递给查询键。您也可以使用与搜索API相同的q参数。
[source,json]
----
{
  "took" : 147, <1>
  "timed_out": false, <2>
  "deleted": 119, <4>
  "batches": 1, <5>
  "version_conflicts": 0, <6>
  "noops": 0, <7>
  "retries": { <8>
    "bulk": 0,
    "search": 0
  },
  "throttled_millis": 0, <9>
  "requests_per_second": -1.0, <10>
  "throttled_until_millis": 0, <11>
  "total": 119, <3>
  "failures" : [ ] <12>
}
---- 
<1> 从整个操作的开始到结束的毫秒数。
<2> 如果在查询执行删除期间执行的任何请求超时，则此标志设置为true。
<3> 已成功处理的文档数。
<4> 已成功删除的文档数。
<5> 通过查询删除回滚的滚动响应数。
<6> 按查询删除的版本冲突数。
<7> 对于按查询删除，此字段始终等于零。 它的存在是为了保持查询删除，按查询更新和重新索引API返回具有相同结构的响应。
<8> 通过查询删除尝试的重试次数。bulk是重试的批量操作数，search是重试的搜索操作数。
<9> 请求睡眠以符合requests_per_second的毫秒数。
<10> 在通过查询删除期间有效执行的每秒请求数。
<11> 在_delete_by_query响应中，此字段应始终等于零。它只在使用Task API时有意义，它表示执行限制的请求时间的毫秒数，为了符合requests_per_second。
<12> 如果在此过程中存在任何不可恢复的错误，则会出现故障数组。如果这不是空的，那么请求因为那些失败而中止。 使用批处理实现查询删除，任何故障都会导致整个进程中止，但当前批处理中的所有故障都将收集到阵列中。您可以使用conflict选项来防止reindex在版本冲突中中止。


_delete_by_query在启动时获取索引的快照，并使用内部版本控制删除它找到的内容。 这意味着如果文档在拍摄快照的时间和处理删除请求之间发生更改，则会出现版本冲突。当版本匹配时，文档将被删除。

NOTE: 由于内部版本控制不支持将值0作为有效版本号，因此无法使用_delete_by_query删除版本等于零的文档，并且将使请求失败。

在_delete_by_query执行期间，顺序执行多个搜索请求，以便找到要删除的所有匹配文档。 每次找到一批文档时，都会执行相应的批量请求以删除所有这些文档。如果搜索或批量请求被拒绝，_delete_by_query依赖于默认策略来重试被拒绝的请求（最多10次，指数退回）。 达到最大重试次数限制会导致_delete_by_query中止，并且在响应失败时返回所有失败。已执行的删除仍然有效。 换句话说，该过程不会回滚，只会中止。 当第一个故障导致中止时，失败的批量请求返回的所有故障都将在failure元素中返回; 因此，可能存在相当多的失败实体。

如果发生版本冲突而你又不想终止执行,可以在url请求参数中设置conflicts=proceed或者在请求体中设置"conflicts":"proceed"
删除索引下面某个type所有的文档:
[source,shell]
----
POST twitter/_doc/_delete_by_query?conflicts=proceed
{
  "query": {
    "match_all": {}
  }
}
----

删除多个索引下面的多个type:
[source,shell]
----
POST twitter,blog/_docs,post/_delete_by_query
{
  "query": {
    "match_all": {}
  }
}
----
如果提供路由，则路由将复制到滚动查询，从而将进程限制为与该路由值匹配的分片:
[source,shell]
----
POST twitter/_delete_by_query?routing=1
{
  "query": {
    "range" : {
        "age" : {
           "gte" : 10
        }
    }
  }
}
----
默认情况下，_delete_by_query使用1000的滚动批次。您可以使用scroll_size URL参数更改批量大小：
[source,shell]
----
POST twitter/_delete_by_query?scroll_size=5000
{
  "query": {
    "term": {
      "user": "kimchy"
    }
  }
}
----

==== URL请求参数
除了像pretty这样的标准参数之外，查询API的删除还支持refresh，wait_for_completion，wait_for_active_shards，timeout和scroll。

发送刷新将在请求完成后刷新查询删除中涉及的所有分片。这与delete API的refresh参数不同，后者只会刷新收到删除请求的分片。与delete API不同，它不支持wait_for。

如果请求包含wait_for_completion = false，则Elasticsearch将执行一些预检检查，启动请求，然后返回可与Tasks API一起使用的任务，以取消或获取任务的状态。Elasticsearch还将在.tasks/task/${taskId}中创建此任务的记录作为文档。可以根据需要删除或者保留。完成后，删除它，以便Elasticsearch可以回收它使用的空间。


wait_for_active_shards控制在继续请求之前必须激活碎片的副本数。timeout指示每个写入请求等待不可用分片可用的时间。 两者都完全适用于Bulk API中的工作方式。 由于_delete_by_query使用滚动搜索，您还可以指定scroll参数来控制“搜索上下文”保持活动的时间，例如?scroll=10m。默认情况下，它是5分钟。

requests_per_second可以设置为任何正十进制数（1.4,6,1000等），并通过在等待时间内填充每个批次来限制查询删除发出批量删除操作的速率。可以通过将requests_per_second设置为-1来禁用限制。

throttling是两个批处理之间的等待时间,也就是每个滚动之间的间隔时间,主要为了满足requests_per_second指标,而让请求发生等待的时间.该填充时间是批大小/requests_per_second-写时间.默认批大小是1000,如果requests_per_second是500:
[source,shell]
----
target_time = 1000 / 500 per second = 2 seconds
wait_time = target_time - write_time = 2 seconds - .5 seconds = 1.5 seconds
----

由于批处理是作为单个_bulk请求发出的，因此大批量大小将导致Elasticsearch创建许多请求，然后等待一段时间再开始下一个集合。 这是“突发”而不是“平滑”。 默认值为-1。

**work with the Task API**
您可以使用TaskAPI查询任何正在运行的删除操作请求的状态：
[source,shell]
----
GET _tasks?detailed=true&actions=*/delete/byquery
----
[source,json]
----
{
  "nodes" : {
    "r1A2WoRbTwKZ516z6NEs5A" : {
      "name" : "r1A2WoR",
      "transport_address" : "127.0.0.1:9300",
      "host" : "127.0.0.1",
      "ip" : "127.0.0.1:9300",
      "attributes" : {
        "testattr" : "test",
        "portsfile" : "true"
      },
      "tasks" : {
        "r1A2WoRbTwKZ516z6NEs5A:36619" : {
          "node" : "r1A2WoRbTwKZ516z6NEs5A",
          "id" : 36619,
          "type" : "transport",
          "action" : "indices:data/write/delete/byquery",
          "status" : {    <1>
            "total" : 6154,
            "updated" : 0,
            "created" : 0,
            "deleted" : 3500,
            "batches" : 36,
            "version_conflicts" : 0,
            "noops" : 0,
            "retries": 0,
            "throttled_millis": 0
          },
          "description" : ""
        }
      }
    }
  }
}

----

<1> 该对象包含实际执行的状态,total是实际执行操作的总数,当其他值相加等于改值时,表明请求执行完成

你也可以通过下面的请求直接查看任务:
[source,shell]
----
GET /_tasks/r1A2WoRbTwKZ516z6NEs5A:36619
----
此API的优点是它与wait_for_completion=false集成，以透明地返回已完成任务的状态。如果任务完成并且在其上设置了wait_for_completion = false，那么它将返回结果或错误字段。此功能的成本是wait_for_completion = false在.tasks/task/${taskId}创建的文档。 您可以删除该文档。

==== Works with the Cancel Task API
可以使用任务取消API取消任何查询删除：
[source,shell]
----
POST _tasks/r1A2WoRbTwKZ516z6NEs5A:36619/_cancel
----
取消应该很快发生，但可能需要几秒钟。 上面的任务状态API将继续列出按查询删除任务的列表，直到此任务检查它已被取消并终止自身。

可以使用_rethrottle API通过查询在正在运行的删除时更改requests_per_second的值：
[source,shell]
----
POST _delete_by_query/r1A2WoRbTwKZ516z6NEs5A:36619/_rethrottle?requests_per_second=-1
----

就像在查询API的删除中设置它一样，requests_per_second可以是-1来禁用限制，也可以是1.7或12之类的任何十进制数来限制到该级别。 加速查询的Rethrottling会立即生效，但重新启动会减慢查询速度，这将在完成当前批处理后生效。这可以防止滚动超时。

按查询删除支持切片滚动以并行化删除过程。 这种并行化可以提高效率，并提供一种方便的方法将请求分解为更小的部分。

通过为每个请求提供切片ID和切片总数，手动切片查询：
[source,shell]
----
POST twitter/_delete_by_query
{
  "slice": {
    "id": 0,
    "max": 2
  },
  "query": {
    "range": {
      "likes": {
        "lt": 10
      }
    }
  }
}
POST twitter/_delete_by_query
{
  "slice": {
    "id": 1,
    "max": 2
  },
  "query": {
    "range": {
      "likes": {
        "lt": 10
      }
    }
  }
}

----
您可以验证哪些适用于：
[source,shell]
----
GET _refresh
POST twitter/_search?size=0&filter_path=hits.total
{
  "query": {
    "range": {
      "likes": {
        "lt": 10
      }
    }
  }
}
----
这导致像这样的合理total：
[source,json]
----
{
  "hits": {
    "total": 0
  }
}
----

== UPDATE API
更新API允许你通过提供的脚本更新文档的内容.该操作从索引中获取文档,然后运行脚本修改文档,然后再重新索引回文档.他使用版本号来区分内容没有变化的文档更新.

注意，此操作仍然意味着文档的完全重新索引，它只是删除了一些网络往返，并减少了get和索引之间版本冲突的可能性。需要启用_source字段才能使此功能正常工作。看下面的例子,我们先索引一个简单的文档:
[source,shell]
----
PUT test/_doc/1
{
    "counter" : 1,
    "tags" : ["red"]
}
----

**使用脚本更新**
我们执行脚本来增加counter字段:
[source,shell]
----
POST test/_doc/1/_update
{
    "script" : {
        "source": "ctx._source.counter += params.count",
        "lang": "painless",
        "params" : {
            "count" : 4
        }
    }
}
----
我们也可以像tags属性中添加一些数据:
[source,shell]
----
POST test/_doc/1/_update
{
    "script" : {
        "source": "ctx._source.tags.add(params.tag)",
        "lang": "painless",
        "params" : {
            "tag" : "blue"
        }
    }
}
----
我们也可以删除tags列表中的某一项:
[source,shell]
----
POST test/_doc/1/_update
{
    "script" : {
        "source": "if (ctx._source.tags.contains(params.tag)) { ctx._source.tags.remove(ctx._source.tags.indexOf(params.tag)) }",
        "lang": "painless",
        "params" : {
            "tag" : "blue"
        }
    }
}
----

除了_source,下面的变量可以可以通过ctx获取:_index,_type,_id,_version,_routing,_now

我们也可以为文档添加新的字段:
[source,shell]
----
POST test/_doc/1/_update
{
    "script" : "ctx._source.new_field = 'value_of_new_field'"
}
----

或者删除文档的一个字段:
[source,shell]
----
POST test/_doc/1/_update
{
    "script" : "ctx._source.remove('new_field')"
}
----

而且，我们甚至可以改变执行的操作。 如果tags字段包含green，此示例将删除doc，否则它不执行任何操作（noop）：
[source,shell]
----
POST test/_doc/1/_update
{
    "script" : {
        "source": "if (ctx._source.tags.contains(params.tag)) { ctx.op = 'delete' } else { ctx.op = 'none' }",
        "lang": "painless",
        "params" : {
            "tag" : "green"
        }
    }
}
----

**更新局部文档**
更新API还支持传递部分文档，该部分文档将合并到现有文档中（简单的递归合并，对象的内部合并，替换核心“键/值”和数组）。要完全替换现有文档，应使用索引API.
以下部分更新会向现有文档添加新字段：
[source,shell]
----
POST test/_doc/1/_update
{
    "doc" : {
        "name" : "new_name"
    }
}
----

如果同时指定了doc和script，则会忽略doc。最好是将部分文档的字段对放在脚本本身中。

**探测noop更新**
如果指定了doc，则其值将与现有_source合并。 默认情况下，如果检测到它们没有更改任何内容会返回“result”：“noop”，如下所示：
[source,shell]
----
POST test/_doc/1/_update
{
    "doc" : {
        "name" : "new_name"
    }
}
----
如果上面的请求在发送之前,name的值是new_name,整个请求会被忽略,响应中的result会返回noop,如下:
[source,json]
----
{
   "_shards": {
        "total": 0,
        "successful": 0,
        "failed": 0
   },
   "_index": "test",
   "_type": "_doc",
   "_id": "1",
   "_version": 7,
   "result": "noop"
}
----

您可以通过设置“detect_noop=false”来禁用此行为,如下所示：
[source,shell]
----
POST test/_doc/1/_update
{
    "doc" : {
        "name" : "new_name"
    },
    "detect_noop": false
}
----

**upsert**
如果文档尚不存在，则upsert元素的内容将作为新文档插入。 如果文档确实存在，那么将执行脚本：
[source,shell]
----
POST test/_doc/1/_update
{
    "script" : {
        "source": "ctx._source.counter += params.count",
        "lang": "painless",
        "params" : {
            "count" : 4
        }
    },
    "upsert" : {
        "counter" : 1
    }
}
----

**scripted_upsert **
无论文档是否存在,你希望脚本都会执行,你可以设置scripted_upsert 为true:
[source,shell]
----
POST sessions/session/dh3sgudg8gsrgl/_update
{
    "scripted_upsert":true,
    "script" : {
        "id": "my_web_session_summariser",
        "params" : {
            "pageViewEvent" : {
                "url":"foo.com/bar",
                "response":404,
                "time":"2014-01-01 12:32"
            }
        }
    },
    "upsert" : {}
}
----

**doc_as_upsert**
将doc_as_upsert设置为true将使用doc的内容作为upsert值，而不是发送部分doc加上upsert文档：
[source,shell]
----
POST test/_doc/1/_update
{
    "doc" : {
        "name" : "new_name"
    },
    "doc_as_upsert" : true
}
----

== UPdate By Query API
_update_by_query的最简单用法只是对索引中的每个文档执行更新而不更改source。这对于获取新属性或其他一些在线映射更改很有用。 这是API：
[source,shell]
----
POST twitter/_update_by_query?conflicts=proceed
----
返回结果如下:
[source,json]
----
{
  "took" : 147,
  "timed_out": false,
  "updated": 120,
  "deleted": 0,
  "batches": 1,
  "version_conflicts": 0,
  "noops": 0,
  "retries": {
    "bulk": 0,
    "search": 0
  },
  "throttled_millis": 0,
  "requests_per_second": -1.0,
  "throttled_until_millis": 0,
  "total": 120,
  "failures" : [ ]
}
----

其他操作同delete by Query API

== Multi Get API 
Multi Get API允许基于索引，类型，（可选）和id（以及可能的路由）获取多个文档。响应包括一个docs数组，其中所有提取的文档按顺序对应于原始的multi-get请求（如果特定get的失败，则包含此错误的对象将包含在响应中）。 成功获取的数据在结构上类似于get API提供的文档。
[source,shell]
----
GET /_mget
{
    "docs" : [
        {
            "_index" : "test",
            "_type" : "_doc",
            "_id" : "1"
        },
        {
            "_index" : "test",
            "_type" : "_doc",
            "_id" : "2"
        }
    ]
}
----
mget端点也可以用于索引（在这种情况下，它在主体中不是必需的）：
[source,shell]
----
GET /test/_mget
{
    "docs" : [
        {
            "_type" : "_doc",
            "_id" : "1"
        },
        {
            "_type" : "_doc",
            "_id" : "2"
        }
    ]
}
----
同样的也支持type:
[source,shell]
----
GET /test/_doc/_mget
{
    "docs" : [
        {
            "_id" : "1"
        },
        {
            "_id" : "2"
        }
    ]
}
----
在这种情况下，可以直接使用ids元素来简化请求：
[source,shell]
----
GET /test/_doc/_mget
{
    "ids" : ["1", "2"]
}
----

**source过滤**
默认,_source字段被返回,与get API相同,你可以使用_source参数过滤结果,也可以使用_source,_source_includes, and _source_excludes:
[source,shell]
----
GET /_mget
{
    "docs" : [
        {
            "_index" : "test",
            "_type" : "_doc",
            "_id" : "1",
            "_source" : false
        },
        {
            "_index" : "test",
            "_type" : "_doc",
            "_id" : "2",
            "_source" : ["field3", "field4"]
        },
        {
            "_index" : "test",
            "_type" : "_doc",
            "_id" : "3",
            "_source" : {
                "include": ["user"],
                "exclude": ["user.location"]
            }
        }
    ]
}

----

为每个文档检索特定存储的字段，类似于Get API的stored_fields参数。 例如：
[source,shell]
----
GET /_mget
{
    "docs" : [
        {
            "_index" : "test",
            "_type" : "_doc",
            "_id" : "1",
            "stored_fields" : ["field1", "field2"]
        },
        {
            "_index" : "test",
            "_type" : "_doc",
            "_id" : "2",
            "stored_fields" : ["field3", "field4"]
        }
    ]
}

----

你也可以在请求参数中指定:
[source,shell]
----
GET /test/_doc/_mget?stored_fields=field1,field2
{
    "docs" : [
        {
            "_id" : "1" 
        },
        {
            "_id" : "2",
            "stored_fields" : ["field3", "field4"] 
        }
    ]
}
----
你也可以在请求地址中指定routing参数:
[source,shell]
----
GET /_mget?routing=key1
{
    "docs" : [
        {
            "_index" : "test",
            "_type" : "_doc",
            "_id" : "1",
            "routing" : "key2"
        },
        {
            "_index" : "test",
            "_type" : "_doc",
            "_id" : "2"
        }
    ]
}
----

== bulk API 
批量API使得在单个API调用中执行许多索引/删除操作成为可能。这可以大大提高索引速度。

REST API端点是/ _bulk，它期望以下换行符分隔JSON（NDJSON）结构：
[source,shell]
----
action_and_meta_data\n
optional_source\n
action_and_meta_data\n
optional_source\n
....
action_and_meta_data\n
optional_source\n
----
最后一行数据必须以换行符\n结尾。 每个换行符前面都有一个回车符\r。 向此端点发送请求时，Content-Type标头应设置为application/x-ndjson。

可能的action是index,create,delete和update.index和create需要source数据另起一行,并且具有与标准索引API的op_type参数相同的语义（create已经存在具有相同索引和类型的文档将失败，而index将添加或替换文档）。delete不期望下一行的source数据，并且具有与标准删除API相同的语义。update期望在下一行指定部分doc，upsert和script及其选项。

如果要为curl提供文本文件输入，则必须使用--data-binary标志而不是plain -d。 后者不保留换行符。 例：
[source,shell]
----
$ cat requests
{ "index" : { "_index" : "test", "_type" : "_doc", "_id" : "1" } }
{ "field1" : "value1" }
$ curl -s -H "Content-Type: application/x-ndjson" -XPOST localhost:9200/_bulk --data-binary "@requests"; echo
{"took":7, "errors": false, "items":[{"index":{"_index":"test","_type":"_doc","_id":"1","_version":1,"result":"created","forced_refresh":false}}]}

----

由于此格式使用文字\n作为分隔符，因此请确保JSON操作和source没有被格式化。 以下是批量命令的正确序列示例：
[source,shell]
----
POST _bulk
{ "index" : { "_index" : "test", "_type" : "_doc", "_id" : "1" } }
{ "field1" : "value1" }
{ "delete" : { "_index" : "test", "_type" : "_doc", "_id" : "2" } }
{ "create" : { "_index" : "test", "_type" : "_doc", "_id" : "3" } }
{ "field1" : "value3" }
{ "update" : {"_id" : "1", "_type" : "_doc", "_index" : "test"} }
{ "doc" : {"field2" : "value2"} }
----
结果如下:
[source,json]
----
{
   "took": 30,
   "errors": false,
   "items": [
      {
         "index": {
            "_index": "test",
            "_type": "_doc",
            "_id": "1",
            "_version": 1,
            "result": "created",
            "_shards": {
               "total": 2,
               "successful": 1,
               "failed": 0
            },
            "status": 201,
            "_seq_no" : 0,
            "_primary_term": 1
         }
      },
      {
         "delete": {
            "_index": "test",
            "_type": "_doc",
            "_id": "2",
            "_version": 1,
            "result": "not_found",
            "_shards": {
               "total": 2,
               "successful": 1,
               "failed": 0
            },
            "status": 404,
            "_seq_no" : 1,
            "_primary_term" : 2
         }
      },
      {
         "create": {
            "_index": "test",
            "_type": "_doc",
            "_id": "3",
            "_version": 1,
            "result": "created",
            "_shards": {
               "total": 2,
               "successful": 1,
               "failed": 0
            },
            "status": 201,
            "_seq_no" : 2,
            "_primary_term" : 3
         }
      },
      {
         "update": {
            "_index": "test",
            "_type": "_doc",
            "_id": "1",
            "_version": 2,
            "result": "updated",
            "_shards": {
                "total": 2,
                "successful": 1,
                "failed": 0
            },
            "status": 200,
            "_seq_no" : 3,
            "_primary_term" : 4
         }
      }
   ]
}

----

端点是/_bulk，/{index}/_ bulk和{index}/{type}/_ bulk。 提供索引或索引/类型时，默认情况下将对未明确提供它们的批量项使用它们。

.update列子
[source,shell]
----
POST _bulk
{ "update" : {"_id" : "1", "_type" : "_doc", "_index" : "index1", "retry_on_conflict" : 3} }
{ "doc" : {"field" : "value"} }
{ "update" : { "_id" : "0", "_type" : "_doc", "_index" : "index1", "retry_on_conflict" : 3} }
{ "script" : { "source": "ctx._source.counter += params.param1", "lang" : "painless", "params" : {"param1" : 1}}, "upsert" : {"counter" : 1}}
{ "update" : {"_id" : "2", "_type" : "_doc", "_index" : "index1", "retry_on_conflict" : 3} }
{ "doc" : {"field" : "value"}, "doc_as_upsert" : true }
{ "update" : {"_id" : "3", "_type" : "_doc", "_index" : "index1", "_source" : true} }
{ "doc" : {"field" : "value"} }
{ "update" : {"_id" : "4", "_type" : "_doc", "_index" : "index1"} }
{ "doc" : {"field" : "value"}, "_source": true}
----

== reindex api

_reindex最基本的用法就是复制一个索引的内容到另一个内容了.如下面的:
[source,shell]
----
POST _reindex
{
  "source": {
    "index": "twitter"
  },
  "dest": {
    "index": "new_twitter"
  }
}
----

NOTE: Reindex不会尝试设置目标索引。它不会复制源索引的设置。 您应该在运行_reindex操作之前设置目标索引，包括设置映射，分片计数，副本等。

NOTE: Reindex要求为源索引中的所有文档启用_source。

就像_update_by_query一样，_reindex获取源索引的快照，但其目标必须是不同的索引，因此不太可能发生版本冲突。dest元素可以像索引API一样配置，以控制乐观并发控制。 只是省略version_type（如上所述）或将其设置为internal将导致Elasticsearch盲目地将文档转储到目标中，覆盖任何碰巧具有相同类型和id的文件：
[source,shell]
----
POST _reindex
{
  "source": {
    "index": "twitter"
  },
  "dest": {
    "index": "new_twitter",
    "version_type": "internal"
  }
}
----
将version_type设置为external将导致Elasticsearch保留源中的版本，创建缺少的任何文档，并更新目标索引中具有旧版本的文档而不是复制源索引中的任何文档：
[source,shell]
----
POST _reindex
{
  "source": {
    "index": "twitter"
  },
  "dest": {
    "index": "new_twitter",
    "version_type": "external"
  }
}
----

设置op_type=create将导致_reindex仅在目标索引中创建缺少的文档。 所有现有文档都会导致版本冲突：
[source,shell]
----
POST _reindex
{
  "source": {
    "index": "twitter"
  },
  "dest": {
    "index": "new_twitter",
    "op_type": "create"
  }
}
----
默认情况下，版本冲突会中止_reindex进程，但您可以通过在请求正文中设置"conflicts": "proceed"来计算它们：
[source,shell]
----
POST _reindex
{
  "conflicts": "proceed",
  "source": {
    "index": "twitter"
  },
  "dest": {
    "index": "new_twitter",
    "op_type": "create"
  }
}
----
你可以在source上添加查询来缩小范围:
[source,shell]
----
POST _reindex
{
  "source": {
    "index": "twitter",
    "type": "_doc",
    "query": {
      "term": {
        "user": "kimchy"
      }
    }
  },
  "dest": {
    "index": "new_twitter"
  }
}
----
源中的索引和类型都可以是列表，允许您在一个请求中从许多源复制。这将从twitter和博客索引中复制_doc和post类型的文档。
[source,shell]
----
POST _reindex
{
  "source": {
    "index": ["twitter", "blog"],
    "type": ["_doc", "post"]
  },
  "dest": {
    "index": "all_together",
    "type": "_doc"
  }
}
----
NOTE: Reindex API不会处理ID冲突，因此最后编写的文档将“获胜”，但顺序通常不可预测，因此依赖此行为并不是一个好主意。 而是使用脚本确保ID是唯一的。

也可以通过设置大小来限制处理文档的数量。 
[source,shell]
----
POST _reindex
{
  "size": 1,
  "source": {
    "index": "twitter"
  },
  "dest": {
    "index": "new_twitter"
  }
}
----

如果你想要twitter索引中的特定文档集，你需要使用sort.排序使滚动效率降低，但在某些情况下，它是值得的。如果可能，请选择更具选择性的查询来进行大小和排序。这会将10000个文件从twitter复制到new_twitter：
[source,shell]
----
POST _reindex
{
  "size": 10000,
  "source": {
    "index": "twitter",
    "sort": { "date": "desc" }
  },
  "dest": {
    "index": "new_twitter"
  }
}
----

source部分支持搜索请求中所有的元素,例如,源文档中一部分字段被返回:
[source,shell]
----
POST _reindex
{
  "source": {
    "index": "twitter",
    "_source": ["user", "_doc"]
  },
  "dest": {
    "index": "new_twitter"
  }
}
----
与_update_by_query一样，_reindex支持修改文档的脚本。与_update_by_query不同，允许脚本修改文档的元数据。这个例子覆盖了源文档的版本：
[source,shell]
----
POST _reindex
{
  "source": {
    "index": "twitter"
  },
  "dest": {
    "index": "new_twitter",
    "version_type": "external"
  },
  "script": {
    "source": "if (ctx._source.foo == 'bar') {ctx._version++; ctx._source.remove('foo')}",
    "lang": "painless"
  }
}
----
就像在_update_by_query中一样，您可以设置ctx.op来更改在目标索引上执行的操作：
. noop:如果脚本确定不必在目标索引中编制索引，请设置ctx.op=“noop”。这种无操作将在响应的noop字段中报告。
. delete:如果脚本确定必须从目标索引中删除文档，请设置ctx.op=“delete”。删除将在响应正文中的deleted字段报告。

将ctx.op设置为其他任何内容都将返回错误，但是可以在ctx中设置任何其他字段。你可以修改:_id,_type,_index,_version,_routing

将_version设置为null或从ctx映射中清除它就像不在索引请求中发送版本一样; 无论目标版本或您在_reindex请求中使用的版本类型如何，它都会导致文档被覆盖在目标索引中。

默认情况下，如果_reindex看到带有路由的文档，则除非脚本更改了路由，否则将保留路由。您可以在dest请求上设置路由以更改此设置：
. keep:将针对每个匹配发送的批量请求的路由设置为匹配上的路由。 这是默认值。
. discard:将针对每个匹配发送的批量请求的路由设置为null。
. =<some text>:将针对每个匹配发送的批量请求的路由设置为=之后的所有文本。

例如，您可以使用以下请求将源索引中的所有文档与公司名称cat复制到dest索引，并将路由设置为cat。
[source,shell]
----
POST _reindex
{
  "source": {
    "index": "source",
    "query": {
      "match": {
        "company": "cat"
      }
    }
  },
  "dest": {
    "index": "dest",
    "routing": "=cat"
  }
}
----
默认情况下，_reindex使用1000的滚动批次。您可以使用source元素中的size字段更改批量大小：
[source,shell]
----
POST _reindex
{
  "source": {
    "index": "source",
    "size": 100
  },
  "dest": {
    "index": "dest",
    "routing": "=cat"
  }
}
----

Reindex还可以通过指定如下管道来使用“Ingest 节点”功能：
[source,shell]
----
POST _reindex
{
  "source": {
    "index": "source"
  },
  "dest": {
    "index": "dest",
    "pipeline": "some_ingest_pipeline"
  }
}
----

Reindex支持从远程Elasticsearch集群重建索引：
[source,shell]
----
POST _reindex
{
  "source": {
    "remote": {
      "host": "http://otherhost:9200",
      "username": "user",
      "password": "pass"
    },
    "index": "source",
    "query": {
      "match": {
        "test": "data"
      }
    }
  },
  "dest": {
    "index": "dest"
  }
}
----

host参数必须包含协议,主机,端口(例如:https://otherhost:9200),路径是可选的(https://otherhost:9200/proxy).username和password也是可选的.当它们存在时_reindex将使用基本身份验证连接到远程Elasticsearch节点。 使用基本身份验证时务必使用https，否则密码将以纯文本格式发送。

必须使用reindex.remote.whitelist属性在elasticsearch.yaml中将远程主机明确列入白名单。 它可以设置为允许的远程主机和端口组合的逗号分隔列表（例如otherhost:9200, nother:9200,127.0.10.*:9200,localhost:*）。白名单忽略Scheme  - 仅使用主机和端口，例如：
[source,shell]
----
reindex.remote.whitelist: "otherhost:9200, another:9200, 127.0.10.*:9200, localhost:*"
----
必须在将协调重新索引的任何节点上配置白名单。

此功能适用于您可能会找到的任何Elasticsearch版本的远程集群。 这应该允许您通过从旧版本的群集重新索引从任何版本的Elasticsearch升级到当前版本。

要启用发送到旧版Elasticsearch的查询，请将查询参数直接发送到远程主机，而不进行验证或修改。

从远程服务器重新索引使用堆上缓冲区，默认最大大小为100mb。 如果远程索引包含非常大的文档，则需要使用较小的批处理大小。 下面的示例将批量大小设置为10，非常非常小。
[source,shell]
----
POST _reindex
{
  "source": {
    "remote": {
      "host": "http://otherhost:9200"
    },
    "index": "source",
    "size": 10,
    "query": {
      "match": {
        "test": "data"
      }
    }
  },
  "dest": {
    "index": "dest"
  }
}
----
也可以使用socket_timeout字段设置远程连接上的套接字读取超时，使用connect_timeout字段设置连接超时。两者都默认为30秒。 此示例将套接字读取超时设置为一分钟，将连接超时设置为10秒：
[source,shell]
----
POST _reindex
{
  "source": {
    "remote": {
      "host": "http://otherhost:9200",
      "socket_timeout": "1m",
      "connect_timeout": "10s"
    },
    "index": "source",
    "query": {
      "match": {
        "test": "data"
      }
    }
  },
  "dest": {
    "index": "dest"
  }
}
----

尽管有上述建议，您可以将_reindex与Painless结合使用以重新索引每日索引，以将新模板应用于现有文档。

假设您的索引由以下文档组成：
[source,shell]
----
PUT metricbeat-2016.05.30/_doc/1?refresh
{"system.cpu.idle.pct": 0.908}
PUT metricbeat-2016.05.31/_doc/1?refresh
{"system.cpu.idle.pct": 0.105}
----
metricbement- *索引的新模板已加载到Elasticsearch中，但它仅适用于新创建的索引。Painless可用于重新索引现有文档并应用新模板。下面的脚本从索引名称中提取日期，并创建一个附加了-1的新索引。 来自metricbeat-2016.05.31的所有数据将重新编入metricbeat-2016.05.31-1。
[source,shell]
----
POST _reindex
{
  "source": {
    "index": "metricbeat-*"
  },
  "dest": {
    "index": "metricbeat"
  },
  "script": {
    "lang": "painless",
    "source": "ctx._index = 'metricbeat-' + (ctx._index.substring('metricbeat-'.length(), ctx._index.length())) + '-1'"
  }
}
----
现在可以在*-1索引中找到先前metricbeat索引中的所有文档。
[source,shell]
----
GET metricbeat-2016.05.30-1/_doc/1
GET metricbeat-2016.05.31-1/_doc/1
----
以前的方法还可以与更改字段名称结合使用，以仅将现有数据加载到新索引中，并根据需要重命名任何字段。

== 词向量
返回特定文档字段中词的信息和统计信息。该文档可以存储在索引中或由用户人工提供。词向量默认是实时的，而不是接近实时的。可以通过将realtime参数设置为false来更改此设置。
[source,shell]
----
GET /twitter/_doc/1/_termvectors
----
（可选）您可以使用url中的参数指定要为其检索信息的字段:
[source,shell]
----
GET /twitter/_doc/1/_termvectors?fields=message
----
或者在请求正文中添加请求的字段（参见下面的示例）。还可以使用通配符以与多匹配查询类似的方式指定字段

可以请求三种类型的值：词信息，词统计和字段统计。 默认情况下，为所有字段返回所有词信息和字段统计信息，但不返回词统计信息。

**词信息**
. 在字段出现的频率
. 词位置
. 词的开始和结束位置
. 词的内容

如果请求的信息未存储在索引中，则将尽可能动态计算。 另外，可以针对甚至不存在于索引中的文档计算术语向量，但是需要由用户提供。

**词统计**
设置term_statistics=true会返回该信息
. 一个术语在所有文档中出现的频率
. 包含当前术语的文档数

默认情况下，不会返回这些值，因为术语统计信息会对性能产生严重影响。

**字段统计**
设置field_statistics =true会返回该信息

. 有多少文档包含此字段
. 此字段中所有术语的文档频率总和
. 此字段中每个术语的总术语频率之和

**Terms过滤**
使用参数过滤器，还可以根据其tf-idf分数过滤返回的术语。 这对于找出文档的良好特征向量可能是有用的。此功能的工作方式与“更喜欢此查询”的第二阶段类似。有关用法，请参见示例5。

支持以下子参数：
. max_num_terms:每个字段必须返回的最大术语数。默认为25。
. min_term_freq:忽略源文档中频率低于此频率的单词。默认为1。
. max_term_freq:忽略源文档中超过此频率的单词。默认为无限制。
. min_doc_freq:忽略至少在这么多文档中没有出现的术语.默认为1。
. max_doc_freq:忽略超过这么多文档中出现的单词。默认为无限制。
. min_word_length:最小字长，低于该字长将被忽略。默认为0。
. max_word_length:最大字长，高于该字长将被忽略。默认为无界（0）。

词和字段统计数据不准确。 删除的文档不会被考虑在内。 仅为请求的文档所在的分片检索信息。因此，术语和字段统计仅用作相对度量，而绝对数字在此上下文中没有意义。 默认情况下，在请求人工文档的术语向量时，随机选择用于获取统计数据的分片。 仅使用路由来命中特定的分片。

**返回存储的词向量**
首先,我们创建索引:
[source,shell]
----
PUT /twitter/
{ "mappings": {
    "_doc": {
      "properties": {
        "text": {
          "type": "text",
          "term_vector": "with_positions_offsets_payloads",
          "store" : true,
          "analyzer" : "fulltext_analyzer"
         },
         "fullname": {
          "type": "text",
          "term_vector": "with_positions_offsets_payloads",
          "analyzer" : "fulltext_analyzer"
        }
      }
    }
  },
  "settings" : {
    "index" : {
      "number_of_shards" : 1,
      "number_of_replicas" : 0
    },
    "analysis": {
      "analyzer": {
        "fulltext_analyzer": {
          "type": "custom",
          "tokenizer": "whitespace",
          "filter": [
            "lowercase",
            "type_as_payload"
          ]
        }
      }
    }
  }
}
----
然后,添加一些文档:
[source,shell]
----
PUT /twitter/_doc/1
{
  "fullname" : "John Doe",
  "text" : "twitter test test test "
}

PUT /twitter/_doc/2
{
  "fullname" : "Jane Doe",
  "text" : "Another twitter test ..."
}
----
以下请求返回文档1（John Doe）中字段文本的所有信息和统计信息：
[source,shell]
----
GET /twitter/_doc/1/_termvectors
{
  "fields" : ["text"],
  "offsets" : true,
  "payloads" : true,
  "positions" : true,
  "term_statistics" : true,
  "field_statistics" : true
}
----
响应:
[source,json]
----
{
    "_id": "1",
    "_index": "twitter",
    "_type": "_doc",
    "_version": 1,
    "found": true,
    "took": 6,
    "term_vectors": {
        "text": {
            "field_statistics": {
                "doc_count": 2,
                "sum_doc_freq": 6,
                "sum_ttf": 8
            },
            "terms": {
                "test": {
                    "doc_freq": 2,
                    "term_freq": 3,
                    "tokens": [
                        {
                            "end_offset": 12,
                            "payload": "d29yZA==",
                            "position": 1,
                            "start_offset": 8
                        },
                        {
                            "end_offset": 17,
                            "payload": "d29yZA==",
                            "position": 2,
                            "start_offset": 13
                        },
                        {
                            "end_offset": 22,
                            "payload": "d29yZA==",
                            "position": 3,
                            "start_offset": 18
                        }
                    ],
                    "ttf": 4
                },
                "twitter": {
                    "doc_freq": 2,
                    "term_freq": 1,
                    "tokens": [
                        {
                            "end_offset": 7,
                            "payload": "d29yZA==",
                            "position": 0,
                            "start_offset": 0
                        }
                    ],
                    "ttf": 2
                }
            }
        }
    }
}

----

未明确存储在索引中的术语向量是在运行中自动计算的。 以下请求将返回文档1中字段的所有信息和统计信息，即使这些条款尚未明确存储在索引中。请注意，对于字段文本，不会重新生成术语。
[source,shell]
----
GET /twitter/_doc/1/_termvectors
{
  "fields" : ["text", "some_field_without_term_vectors"],
  "offsets" : true,
  "positions" : true,
  "term_statistics" : true,
  "field_statistics" : true
}
----

还可以为人工文档生成术语向量，即用于索引中不存在的文档。 例如，以下请求将返回与示例1中相同的结果。使用的映射由索引和类型确定。
[source,shell]
----
GET /twitter/_doc/_termvectors
{
  "doc" : {
    "fullname" : "John Doe",
    "text" : "twitter test test test"
  }
}
----
另外，可以通过使用per_field_analyzer参数来提供与现场不同的分析器。 这对于以任何方式生成术语向量是有用的，尤其是在使用人工文档时。 当为已经存储术语向量的场提供分析器时，将重新生成术语向量。
[source,shell]
----
GET /twitter/_doc/_termvectors
{
  "doc" : {
    "fullname" : "John Doe",
    "text" : "twitter test test test"
  },
  "fields": ["fullname"],
  "per_field_analyzer" : {
    "fullname": "keyword"
  }
}
----
响应:
[source,json]
----
{
  "_index": "twitter",
  "_type": "_doc",
  "_version": 0,
  "found": true,
  "took": 6,
  "term_vectors": {
    "fullname": {
       "field_statistics": {
          "sum_doc_freq": 2,
          "doc_count": 4,
          "sum_ttf": 4
       },
       "terms": {
          "John Doe": {
             "term_freq": 1,
             "tokens": [
                {
                   "position": 0,
                   "start_offset": 0,
                   "end_offset": 8
                }
             ]
          }
       }
    }
  }
}

----
最后，可以根据他们的tf-idf分数过滤返回的术语。 在下面的示例中，我们从具有给定“plot”字段值的人工文档中获得三个最“有趣”的关键字。 请注意，关键字“Tony”或任何停用词不是响应的一部分，因为它们的tf-idf必须太低。
[source,shell]
----
GET /imdb/_doc/_termvectors
{
    "doc": {
      "plot": "When wealthy industrialist Tony Stark is forced to build an armored suit after a life-threatening incident, he ultimately decides to use its technology to fight against evil."
    },
    "term_statistics" : true,
    "field_statistics" : true,
    "positions": false,
    "offsets": false,
    "filter" : {
      "max_num_terms" : 3,
      "min_term_freq" : 1,
      "min_doc_freq" : 1
    }
}
----
结果:
[source,json]
----
{
   "_index": "imdb",
   "_type": "_doc",
   "_version": 0,
   "found": true,
   "term_vectors": {
      "plot": {
         "field_statistics": {
            "sum_doc_freq": 3384269,
            "doc_count": 176214,
            "sum_ttf": 3753460
         },
         "terms": {
            "armored": {
               "doc_freq": 27,
               "ttf": 27,
               "term_freq": 1,
               "score": 9.74725
            },
            "industrialist": {
               "doc_freq": 88,
               "ttf": 88,
               "term_freq": 1,
               "score": 8.590818
            },
            "stark": {
               "doc_freq": 44,
               "ttf": 47,
               "term_freq": 1,
               "score": 9.272792
            }
         }
      }
   }
}
----

== multi termvectors API
多termvectors API允许一次获得多个termvectors。从中检索术语向量的文档由索引，类型和id指定。但是文件也可以在请求本身中人为提供。

响应包括一个docs数组，其中包含所有获取的termvectors，每个元素都具有termvectors API提供的结构。这是一个例子：
[source,shell]
----
POST /_mtermvectors
{
   "docs": [
      {
         "_index": "twitter",
         "_type": "_doc",
         "_id": "2",
         "term_statistics": true
      },
      {
         "_index": "twitter",
         "_type": "_doc",
         "_id": "1",
         "fields": [
            "message"
         ]
      }
   ]
}
----

[source,shell]
----
POST /twitter/_mtermvectors
{
   "docs": [
      {
         "_type": "_doc",
         "_id": "2",
         "fields": [
            "message"
         ],
         "term_statistics": true
      },
      {
         "_type": "_doc",
         "_id": "1"
      }
   ]
}

----

[source,shell]
----
POST /twitter/_doc/_mtermvectors
{
   "docs": [
      {
         "_id": "2",
         "fields": [
            "message"
         ],
         "term_statistics": true
      },
      {
         "_id": "1"
      }
   ]
}
----

[source,shell]
----
POST /twitter/_doc/_mtermvectors
{
    "ids" : ["1", "2"],
    "parameters": {
        "fields": [
                "message"
        ],
        "term_statistics": true
    }
}
----

[source,shell]
----
POST /_mtermvectors
{
   "docs": [
      {
         "_index": "twitter",
         "_type": "_doc",
         "doc" : {
            "user" : "John Doe",
            "message" : "twitter test test test"
         }
      },
      {
         "_index": "twitter",
         "_type": "_doc",
         "doc" : {
           "user" : "Jane Doe",
           "message" : "Another twitter test ..."
         }
      }
   ]
}
----

=== ?refresh
Index，Update，Delete和Bulk API支持设置refresh，以控制此请求所做的更改何时对搜索可见。 这些是允许的值：

. Empty string or true:在操作发生后立即刷新相关的主分片和副本分片（而不是整个索引），以便更新的文档立即显示在搜索结果中。 只有在从索引和搜索角度进行仔细考虑并验证它不会导致性能不佳之后，才能进行此操作。

. wait_for:等待该操作对所有操作可见之后,返回成功响应.这不会强制立即刷新，而是等待刷新请求触发。elastic自动会自动刷新变更的分片,默认一秒,可以通过index.refresh_interval修改.

. false:不采取与刷新相关的操作。 此请求所做的更改将在请求返回后的某个时间点可见。

除非您有充分的理由等待更改变为可见，否则请始终使用refresh = false，或者，因为这是默认值，只需将刷新参数保留在URL之外。 这是最简单，最快捷的选择

如果您必须让请求所做的更改与请求同步显示，那么您必须选择在Elasticsearch上添加更多负载（true）并等待响应更长时间（wait_for）。 以下是应该通知该决定的几点：

The more changes being made to the index the more work wait_for saves compared to true. In the case that the index is only changed once every index.refresh_interval then it saves no work.
true creates less efficient indexes constructs (tiny segments) that must later be merged into more efficient index constructs (larger segments). Meaning that the cost of true is paid at index time to create the tiny segment, at search time to search the tiny segment, and at merge time to make the larger segments.
Never start multiple refresh=wait_for requests in a row. Instead batch them into a single bulk request with refresh=wait_for and Elasticsearch will start them all in parallel and return only when they have all finished.
If the refresh interval is set to -1, disabling the automatic refreshes, then requests with refresh=wait_for will wait indefinitely until some action causes a refresh. Conversely, setting index.refresh_interval to something shorter than the default like 200ms will make refresh=wait_for come back faster, but it’ll still generate inefficient segments.
refresh=wait_for only affects the request that it is on, but, by forcing a refresh immediately, refresh=true will affect other ongoing requests. In general, if you have a running system you don’t wish to disturb then refresh=wait_for is a smaller modification.


创建文档后,立即刷新,使搜索可见.
[source,shell]
----
PUT /test/_doc/1?refresh
{"test": "test"}
PUT /test/_doc/2?refresh=true
{"test": "test"}
----

这些将创建一个文档而不做任何使搜索可见的其他操作.
[source,shell]
----
PUT /test/_doc/3
{"test": "test"}
PUT /test/_doc/4?refresh=false
{"test": "test"}
----

这将创建一个文档并等待它对搜索可见：
[sourcee,shell]
----
PUT /test/_doc/4?refresh=wait_for
{"test": "test"}
----

== 乐观锁

Elasticsearch是分布式的。 创建，更新或删除文档时，必须将新版本的文档复制到群集中的其他节点。 Elasticsearch也是异步和并发的，这意味着这些复制请求是并行发送的，并且可能不按顺序到达目的地。 Elasticsearch需要一种方法来确保旧版本的文档永远不会覆盖较新的版本。

为确保较旧版本的文档不会覆盖较新版本，对文档执行的每个操作都会由协调该更改的主分片分配序列号。 每次操作都会增加序列号，因此保证较新的操作具有比旧操作更高的序列号。 然后，Elasticsearch可以使用序列号操作来确保更新的文档版本永远不会被分配给它的序列号更小的更改覆盖。

例如，以下索引命令将创建一个文档，并为其分配一个初始序列号和主要术语：
[source,shell]
----
PUT products/_doc/1567
{
    "product" : "r2d2",
    "details" : "A resourceful astromech droid"
}
----
您可以在响应的_seq_no和_primary_term字段中查看指定的序列号和主要术语：
[source,shell]
----
{
    "_shards" : {
        "total" : 2,
        "failed" : 0,
        "successful" : 1
    },
    "_index" : "products",
    "_type" : "_doc",
    "_id" : "1567",
    "_version" : 1,
    "_seq_no" : 362,
    "_primary_term" : 2,
    "result" : "created"
}
----

Elasticsearch会记录上次操作的序列号和主要术语，以更改它存储的每个文档。 序列号和主要术语在GET API的响应中的_seq_no和_primary_term字段中返回：
[source,shell]
----
GET products/_doc/1567
----
[source,json]
----
{
    "_index" : "products",
    "_type" : "_doc",
    "_id" : "1567",
    "_version" : 1,
    "_seq_no" : 362,
    "_primary_term" : 2,
    "found": true,
    "_source" : {
        "product" : "r2d2",
        "details" : "A resourceful astromech droid"
    }
}
----
序列号和主要术语唯一标识某次更改。 通过记下返回的序列号和主要术语，您可以确保在您检索到文档后没有其他调用更改该文档。 这是通过设置Index API或Delete API的if_seq_no和if_primary_term参数来完成的。

例如，以下索引调用将确保向文档添加tags内容，而不会丢失另一个调用对tag的修改
[source,shell]
----
PUT products/_doc/1567?if_seq_no=362&if_primary_term=2
{
    "product" : "r2d2",
    "details" : "A resourceful astromech droid",
    "tags": ["droid"]
}
----