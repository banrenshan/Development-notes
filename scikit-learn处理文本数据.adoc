= scikit-learn处理文本数据

本指南的主要目标是通过一项实际的任务来认识scikit-learn工具。

== 加载数据

[source,python]
----
from sklearn.datasets import load_files
dataset = load_files(r'D:\python_code\untitled\data\ChnSentiCorp情感分析酒店评论', encoding='UTF-8')
----
我们来看一下加载目录的结构
----
        ChnSentiCorp情感分析酒店评论/
            正面/
                pos_1.txt
                pos_2.txt
                ...
                pos_42.txt
            负面/
                neg_43.txt
                neg_44.txt
----

dataset对象是加载后的数据集，上面有三个关键的属性：data、target_names(分类标签)、target(分类标签的数字表示)。
在上面的例子中，target_names的值是[正面,负面],target是[0,1],可以通过target_names[target[i]]来获取target对应的分类标签

== 抽取文本特征

为了在文本文档上执行机器学习，我们首先需要将文本内容转换为数字特征向量。

=== 词袋

最直观的方式就是用词袋表示：
* 为训练集文档中出现的每个词分配一个固定的整数id（例如，建立索引词典）。
* 对于每个文档#i，计数每个词w的出现次数并将其作为特征#j的值存储在X [i，j]中，其中j是词典中的词w的索引

这些词语表示意味着n_features在语料库中不同单词的数量：该数字通常大于100,000。

如果n_samples == 10000，将X存储为类型为float32的numpy数组需要在内存中有10000 x 100000 x 4字节= 4GB，这在当今计算机上几乎不可管理。

幸运的是，X中的大多数值都是零，因为对于给定的文档，最多上千个不同的单词会被使用。 出于这个原因，我们说，词袋通常是高维稀疏数据集。我们可以通过仅将特征向量的非零部分存储在内存中来节省大量内存。

scipy.sparse矩阵就是这样做的数据结构，scikit-learn内置了对这些结构的支持。

=== 用scikit-learn标记文本

scikit-learn提供了文本预处理，文本标记和过滤停用词的高级组件，该组件能够构建特征字典并将文档转换为特征向量。

统计词频是一个很好的开始，但存在一个问题：在谈论相同的主题，较长的文档的平均计数值会比较短的文档更高。为避免这些潜在的差异，可以将文档中每个单词的出现次数除以文档中的单词总数：这些新功能称为词频tf。

在tf之上的另一个改进是缩减语料库文档中经常出现的词语的权重。这种缩减称为词频-逆文件频率(tf-idf)。
[source,python]
----
>>> from sklearn.feature_extraction.text import TfidfTransformer
>>> tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)
>>> X_train_tf = tf_transformer.transform(X_train_counts)
>>> X_train_tf.shape
(2257, 35788)
----
在上面的示例代码中，我们首先使用fit（..）方法来将我们的估计量与数据拟合，然后使用transform（..）方法将我们的计数矩阵转换为tf-idf表示。可以将这两个步骤组合在一起，以更快地达到相同的最终结果。 这是通过使用fit_transform（..）方法完成的，如下所示，并且如前一节中的注释中所述：
[source,python]
----
>>> tfidf_transformer = TfidfTransformer()
>>> X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)
>>> X_train_tfidf.shape
(2257, 35788)
----

== 训练分类器
现在我们有了我们的功能，我们可以训练一个分类器来尝试预测帖子的类别。 我们从一个朴素的贝叶斯分类器开始，它为此任务提供了一个很好的基线。 scikit-learn包含这个分类器的几个变种; 最适合单词计数的是多项式变体：
[source,python]
----
>>> from sklearn.naive_bayes import MultinomialNB
>>> clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)
----
为了尝试预测新文档的结果，我们需要使用与以前几乎相同的特征提取链来提取特征。 区别在于我们在变换器上调用transform 而不是fit_transform，因为它们已经适合训练集：
[source,python]
----
>>> docs_new = ['God is love', 'OpenGL on the GPU is fast']
>>> X_new_counts = count_vect.transform(docs_new)
>>> X_new_tfidf = tfidf_transformer.transform(X_new_counts)

>>> predicted = clf.predict(X_new_tfidf)

>>> for doc, category in zip(docs_new, predicted):
...     print('%r => %s' % (doc, twenty_train.target_names[category]))
...
'God is love' => soc.religion.christian
'OpenGL on the GPU is fast' => comp.graphics
----

== 构建管道
为了使vectorizer => transformer =>classifier 更易于使用，scikit-learn提供了一个类似于复合分类器的Pipeline类：
[source,python]
----
>>> from sklearn.pipeline import Pipeline
>>> text_clf = Pipeline([('vect', CountVectorizer()),
...                      ('tfidf', TfidfTransformer()),
...                      ('clf', MultinomialNB()),
... ])
----
名称vect，tfidf和clf（分类器）是任意的。 我们将在下面的参数优化部分看到它们的用法。 现在我们可以用一个命令来训练模型：
[source,python]
----
>>> text_clf.fit(twenty_train.data, twenty_train.target)  
Pipeline(...)
----

== 评估训练模型
评估模型的预测准确性同样简单：
[source,python]
----
>>> import numpy as np
>>> twenty_test = fetch_20newsgroups(subset='test',
...     categories=categories, shuffle=True, random_state=42)
>>> docs_test = twenty_test.data
>>> predicted = text_clf.predict(docs_test)
>>> np.mean(predicted == twenty_test.target)            
0.834...
----
即，我们达到了83.4％的准确度。 让我们看看我们是否能够用线性支持向量机（SVM）做得更好，SVM被广泛认为是最好的文本分类算法之一（尽管它也比朴素贝叶斯稍慢）。 我们可以通过将不同的分类器对象插入我们的管道来更改学习者：
[source,python]
----
>>> from sklearn.linear_model import SGDClassifier
>>> text_clf = Pipeline([('vect', CountVectorizer()),
...                      ('tfidf', TfidfTransformer()),
...                      ('clf', SGDClassifier(loss='hinge', penalty='l2',
...                                            alpha=1e-3, random_state=42,
...                                            max_iter=5, tol=None)),
... ])
>>> text_clf.fit(twenty_train.data, twenty_train.target)  
Pipeline(...)
>>> predicted = text_clf.predict(docs_test)
>>> np.mean(predicted == twenty_test.target)            
0.912...
----
scikit-learn进一步提供了更详细的结果性能分析工具：
[source,python]
----
>>> from sklearn import metrics
>>> print(metrics.classification_report(twenty_test.target, predicted,
...     target_names=twenty_test.target_names))
...                                         
                        precision    recall  f1-score   support

           alt.atheism       0.95      0.81      0.87       319
         comp.graphics       0.88      0.97      0.92       389
               sci.med       0.94      0.90      0.92       396
soc.religion.christian       0.90      0.95      0.93       398

           avg / total       0.92      0.91      0.91      1502


>>> metrics.confusion_matrix(twenty_test.target, predicted)
array([[258,  11,  15,  35],
       [  4, 379,   3,   3],
       [  5,  33, 355,   3],
       [  5,  10,   4, 379]])
----
正如预期的那样，混淆矩阵表明，来自atheism 和christian 新闻组的帖子比comp.graphics容易被混淆。

== 优化模型参数

我们已经在TfidfTransformer中遇到了一些参数，例如use_idf。分类器也倾向于有很多参数;例如MultinomialNB包含一个平滑参数alpha，而SGDClassifier在目标函数中有惩罚参数alpha和可配置的损失和惩罚项（请参阅模块文档，或使用Python帮助函数来获取这些描述）。

我们可以给参数设置连续值进行详尽搜索来获取最佳参数，而不是调整链中各个组件的参数。 我们尝试使用单词或双字母的所有分类器，使用或不使用idf，并对线性SVM使用0.01或0.001的惩罚参数：
[source,python]
----
>>> from sklearn.model_selection import GridSearchCV
>>> parameters = {'vect__ngram_range': [(1, 1), (1, 2)],
...               'tfidf__use_idf': (True, False),
...               'clf__alpha': (1e-2, 1e-3),
... }
----
显然，这种彻底的搜索可能很昂贵。 如果我们拥有多个CPU核心，我们可以告诉网格搜索者与n_jobs参数并行地尝试这八个参数组合。 如果我们给这个参数的值为-1，网格搜索将检测安装了多少核心并全部使用它们：
[source,python]
----
>>> gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)
----
网格搜索实例的行为就像一个正常的scikit-learn模型。 我们在训练数据的较小子集上执行搜索以加速计算：
----
>>> gs_clf = gs_clf.fit(twenty_train.data[:400], twenty_train.target[:400])
----
调用GridSearchCV对象fit方法获取分类器来预测结果：
----
>>> twenty_train.target_names[gs_clf.predict(['God is love'])[0]]
'soc.religion.christian'
----
对象的best_score_和best_params_属性存储最佳平均分数和与该分数对应的参数设置：
----
>>> gs_clf.best_score_                                  
0.900...
>>> for param_name in sorted(parameters.keys()):
...     print("%s: %r" % (param_name, gs_clf.best_params_[param_name]))
...
clf__alpha: 0.001
tfidf__use_idf: True
vect__ngram_range: (1, 1)
----
在gs_clf.cv_results_上可以找到更详细的搜索摘要。

可以将cv_results_参数作为DataFrame轻松导入到pandas中，以供进一步检查。

